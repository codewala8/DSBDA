{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "de50d3ba-da78-448d-ba93-cc28719b6d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required Imports\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import string\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3b7c580b-62ec-43b0-b366-0e0e53e0599e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download necessary NLTK data\n",
    "\n",
    "nltk. download('punkt')\n",
    "nltk. download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "18122742-62f1-4841-abc9-71ee5f005e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample document\n",
    "\n",
    "corpus = \"I am Siddhi Horane from sinhgad institute of technology and science lonavala .\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b5063246-ba41-4bd8-b8cc-c96675e0ea9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: ['I', 'am', 'Siddhi', 'Horane', 'from', 'sinhgad', 'institute', 'of', 'technology', 'and', 'science', 'lonavala', '.']\n"
     ]
    }
   ],
   "source": [
    " # Step 1: Tokenization\n",
    "\n",
    "tokens = word_tokenize(corpus)\n",
    "print(\"Tokens:\", tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "59b987f2-2320-434e-beb8-b1a70f54d5c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 2 : POS Tagging\n",
    "\n",
    "from nltk import pos_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4309a5f2-4ee7-42b6-a864-0054f90969e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('I', 'PRP'), ('am', 'VBP'), ('Siddhi', 'NNP'), ('Horane', 'NNP'), ('from', 'IN'), ('sinhgad', 'JJ'), ('institute', 'NN'), ('of', 'IN'), ('technology', 'NN'), ('and', 'CC'), ('science', 'NN'), ('lonavala', 'NN'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "tokens = word_tokenize(corpus)\n",
    "print(pos_tag(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "65de5fb7-399e-4061-b1f2-9574874fd0f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned Words: ['siddhi', 'horane', 'sinhgad', 'institute', 'technology', 'science', 'lonavala']\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Remove stop words and punctuation\n",
    "\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "cleaned = [word.lower() for word in tokens if word.lower() not in stop_words and word not in string.punctuation]\n",
    "print(\"Cleaned Words:\", cleaned)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "611b44c2-fe19-4a36-bbb5-aa097b48ca88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stemmed Words: ['siddhi', 'horan', 'sinhgad', 'institut', 'technolog', 'scienc', 'lonavala']\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Stemming\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "stemmed = [stemmer.stem(word) for word in cleaned]\n",
    "print(\"Stemmed Words:\", stemmed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0d142278-efde-4e40-8b44-1e46a64b4c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: TF-IDF on two simple sentences\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5081b3d7-7fdc-483f-8ec4-ed00fe8cdd6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample documents\n",
    "\n",
    "corpus = [\n",
    "    \"Siddhi Horane is a computer engineering student.\",\n",
    "    \"she is from sinhgad institute of technology lonavala.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f729922a-7f9b-4d4d-a0aa-a1a7cd3054c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize vectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "01ef2af8-51df-4da5-83ad-a5c0bfd9608f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'siddhi': 9, 'horane': 3, 'is': 5, 'computer': 0, 'engineering': 1, 'student': 11, 'she': 8, 'from': 2, 'sinhgad': 10, 'institute': 4, 'of': 7, 'technology': 12, 'lonavala': 6}\n"
     ]
    }
   ],
   "source": [
    "# Fit and transform the documents\n",
    "\n",
    "matrix = vectorizer.fit_transform(corpus)\n",
    "print(vectorizer.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "00b4edd8-2d02-4938-960a-2eb0ce67ecef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 11)\t0.42615959880289433\n",
      "  (0, 9)\t0.42615959880289433\n",
      "  (0, 5)\t0.3032160644503863\n",
      "  (0, 3)\t0.42615959880289433\n",
      "  (0, 1)\t0.42615959880289433\n",
      "  (0, 0)\t0.42615959880289433\n",
      "  (1, 12)\t0.3649964681447582\n",
      "  (1, 10)\t0.3649964681447582\n",
      "  (1, 8)\t0.3649964681447582\n",
      "  (1, 7)\t0.3649964681447582\n",
      "  (1, 6)\t0.3649964681447582\n",
      "  (1, 5)\t0.25969799324016246\n",
      "  (1, 4)\t0.3649964681447582\n",
      "  (1, 2)\t0.3649964681447582\n"
     ]
    }
   ],
   "source": [
    "# Step 6: Show TF-IDF values\n",
    "\n",
    "tfidf_matrix = vectorizer.transform(corpus)\n",
    "print(tfidf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e228a6d7-0d12-407e-a1ef-1e587f2cf7bb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
